---
title: "Exercise Quality Prediction"
author: "Abimbola M"
date: "February 26, 2015"
output: 
  html_document:
    theme: united
    highlight: tango
    keep_md: true
---

### Executive Summary

Using data from 4 accelerometers on 6 participants, a model with 99.7% accuracy was used to predict the quality of barbell lifts in 20 test subjects.    

  - Some of the data were discarded due to missing information: variables with no data, Variables with identifiers and timestamps, and variables with greater than 80% correlation. The remaining data with 39 predictors were used for the rest of the analysis.
  - The labelled training set was further divided into a training and testing sets. 3 models were built and compared using their performance on the labelled testing set (a subset of the original training set). The random forest model outperforms boosting trees and support vector machine and very comparable with an ensemble model of the three previously mentioned models.
  - Classification of unlabelled test set was done with the random forest model. The quality prediction for each subject is the same for all the models and only 7 out the 20 subjects perfomed the lift exactly to specification.


### Background

Devices such as Jawbone Up, Nike FuelBand, and Fitbit collect large amount of data about personal activity. This information is popularly used to quantitfy particular activities but rarely the quality of such activities. This project uses data from accelerometers on the belt,forearm,arm, and the dumbell of 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways.  

The data and more information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section the Weight Lifting Exercise Dataset).
The datasets can be downloaded here: [training](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and [test](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).

```{r setoptions, echo=FALSE} 
knitr::opts_chunk$set(fig.width=8, fig.height=4, fig.path='Figs/', warnings=FALSE, message=FALSE, comment="", width=20)
```

### Data Cleaning

Load caret and doParallel libraries.
```{r}
library(caret)
library(doParallel)
```

Load datasets 
```{r warning=FALSE, message=FALSE}
train <- read.csv("pml-training.csv",sep=",", header=TRUE, stringsAsFactors=TRUE, 
                  na.string=c("NA","#DIV/0!"))

test <- read.csv("pml-testing.csv",sep=",", header=TRUE, stringsAsFactors=TRUE,
                 na.string=c("NA","#DIV/0!"))
```

Check for missing data

```{r}
y <- apply(train,2,function(x){sum(is.na(x))})
table(y)
```

The training dataset has 19622 observations and 160 variables and only 60 variables have no missing data for all the observations; remove the variables with missing data. 

```{r}
## Remove columns with missing variables
train1 <- train[colSums(is.na(train)) == 0]
```

Examine the data structure

```{r results='hide'}
str(train1); names(train1)
## Remove columns with identifiers, timestamps, and redundant dataset.
train1 <- train1[,-c(1:7)] 
rm(train)
```

There are 53 variables left. For compuational efficiency, remove features that are higly correlated

```{r}
## Identify correlated predictors
descrCor <- cor(train1[,-53]) #exclude activity labels
highCorr <- sum(abs(descrCor[upper.tri(descrCor)]) > .80)
highCorr
```

Remove these 19 features that are higly correlated

```{r}
highlyCorDescr <- findCorrelation(descrCor, cutoff = .80)
train1 <- train1[,-highlyCorDescr]; dim(train1)
```

In total 40 variables, including the dependent variable, are left for model training. The higly correlated variables could have been kept since they don't affect tree modeling significantly, but I removed them for computational efficiency.

### Modeling

Split the training set into two. I'm taking advantage of this fairly large set to check the performance of my models before the final test set prediction.

```{r}
set.seed(998)
inTraining <- createDataPartition(train1$classe, p = .75, list = FALSE)
training <- train1[inTraining,]
testing  <- train1[-inTraining,]
```

Generate seeds for resampling.

```{r warning=FALSE,message=FALSE}
#create a list of seed
set.seed(123)
seeds <- vector(mode="list",length=11)
for(i in 1:10) seeds[[i]] <- sample.int(1000,8)
seeds[[11]] <- sample.int(1000,1)
```

Register a backend for parallel computation

```{r}
cl <- makeCluster(detectCores())
registerDoParallel(cl)
```

My control parameters for training includes a YeoJohnson transformation (some variables have zeros and negative values) and removal of variables with near-zero-variance. The latter because the variables with lots of zeros and few unique values may have undue influence on the modeling. 

```{r}
## Control parameters
ctrl = trainControl(method="cv",seeds=seeds,allowParallel=TRUE, 
                    preProc=c("center","scale", "YeoJohnson", "nzv"))
```



####*Random Forest Model*
```{r randomForest, cache=TRUE}
rfGrid <- expand.grid(mtry=c(1,2,4,6))
Model1 <- train(classe ~ ., method = "rf", tuneGrid=rfGrid, trControl=ctrl, data=training)
Model1$finalModel
```

The final model has 500 trees and 6 variables tried at each split for prediction with an OOB estimate of 0.61%. This is pretty impressive! It leaves little room for improvement, but I will play around with other models.


```{r plot1, echo=FALSE}
## Variable importance
plot(varImp(Model1, scale=FALSE), top=10)
```


####*Generalized boosted model*
```{r boosting, cache=TRUE}
gbGrid <- expand.grid(interaction.depth = c(1,2,4), n.trees = c(100,150,200), shrinkage = 0.1, n.minobsinnode = 20)
Model2 <- train(classe ~ ., method="gbm", data=training, trControl=ctrl, tuneGrid=gbGrid, verbose=FALSE)
Model2
```

The final model has 200 trees and an interaction depth of 4 with 97.8% accuracy, lower than the random forest model by 2%

```{r plot2, echo=FALSE}
## Variable importance
plot(varImp(Model2, scale=FALSE), top=10)
```


####*Support Vector Machine*
```{r SVM,cache=TRUE}
Model3 <- train(classe ~ ., data = training, method = "svmRadial", tuneLength=4, trControl=ctrl)
Model3
```

The final model has a cost function of 2 and accuracy of 94.9%. Increasing the cost function may actually increase its accuracy but I will keep it this way to avoid overfitting.

```{r plot3,fig.width=8,fig.height=5,echo=FALSE}
## Variable importance
plot(varImp(Model3, scale=FALSE), top=10)
```

I wish I have a better way to reduce this plot to one box since the variables importance are the same across all classes of activity.

The top 10 variables on the importance scale are the same for models 1 and 2. Only 5 of the top 10 in the support vector model (model3) are shared with the other models. 3 of these 5 variables are the same: magnetdumbell in three dimensional plane(x,y,z). The top 3 important variables are the same for the the first 2 models and 1 of the 3 variables, pitchforearm, is the most important variable in Model3 prediction 

####*Discriminant Model*
For curiosity sake, let's see how discriminant models perform. I already have a model with an OOB error rate of 0.61% so I am not expecting much. 

```{r lda,echo=FALSE,cache=TRUE}
plda <- train(classe ~ ., method="lda", data=training)
plda
```

```{r qda,cache=TRUE,echo=FALSE}
pqda <- train(classe ~ ., method="qda", data=training)
pqda
```

```{r echo=FALSE}
##Close parallel connection
stopCluster(cl)
```

Unsurprisingly, the quadratic discriminant model performed better than the linear and both performed poorly compared to the three models above. They will not be considered further.

####*Perfomance testing* 
```{r}
Pred1 <- predict(Model1, testing)
confusionMatrix(Pred1, testing$classe)$overall

Pred2 <- predict(Model2, testing)
confusionMatrix(Pred2, testing$classe)$overall

Pred3 <- predict(Model3, testing)
confusionMatrix(Pred3, testing$classe)$overall
```

The accuracy of the three models above on the testing set is the same as its performance on the training set. Time to answer the main question: how well are the 20 test subjects performing the barbell lifts?

####*Prediction of the unlabelled 20 test subjects*
The model with the best OOB estimate, random forest model, will be used to make the classification. 
```{r}
##Test subjects classification
table(predict(Model1, test))
```

Since the accuracy of the 3 models are quite high, I wonder if they make the same prediction for each subject.

```{r comparison}
identical(predict(Model1, test), predict(Model2, test), predict(Model3, test))
```

### Conclusion
Only 7 of the 20 test subjects performed the exercise exactly to specification.
